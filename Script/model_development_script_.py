# -*- coding: utf-8 -*-
"""Model_Development_Script .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n92rF4apFYPsVkXDKEkKJG2pGtDbv9Sm
"""

# Importing all necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

#Load Dataset from Goggle drive

from google.colab import drive
drive.mount("/content/drive")
df = pd.read_csv("/content/drive/MyDrive/Data/fraudTrain.csv")

#Removal of the column: Unnamed: 0
df2 = df.drop(['Unnamed: 0'], axis=1, inplace = True)

# --- DATE AND TIME FEATURE ENGINEERING ---
# Convert to datetime if not already
df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'])

# Extract date and time components
df['year'] = df['trans_date_trans_time'].dt.year
df['month'] = df['trans_date_trans_time'].dt.month
df['day'] = df['trans_date_trans_time'].dt.day
df['day_of_week'] = df['trans_date_trans_time'].dt.day_name()
df['hour'] = df['trans_date_trans_time'].dt.hour

# Create a weekend flag (1 for weekend, 0 for weekday)
df['is_weekend'] = (df['trans_date_trans_time'].dt.dayofweek >= 5).astype(int)

# Optional: categorize hour into parts of the day
def get_time_of_day(hour):
    if 5 <= hour < 12:
        return 'Morning'
    elif 12 <= hour < 17:
        return 'Afternoon'
    elif 17 <= hour < 21:
        return 'Evening'
    else:
        return 'Night'

df['time_of_day'] = df['hour'].apply(get_time_of_day)

df.head()

#Drop trans_date_trans_time
df = df.drop(columns=['trans_date_trans_time'])

# Transaction amount feature engineering : log transform and extreme-value flags
import numpy as np


# 1) Create a safe log-transformed amount (handles zeros)
df['log_amt'] = np.log1p(df['amt'])   # log1p = log(1 + amt)


# 2) Define extreme thresholds (5th and 95th percentiles)
low_threshold = df['amt'].quantile(0.05)
high_threshold = df['amt'].quantile(0.95)


# 3) Create binary flags for very low and very high amounts
df['is_low_amount'] = (df['amt'] < low_threshold).astype(int)
df['is_high_amount'] = (df['amt'] > high_threshold).astype(int)


# 4) Quick preview of the new columns
df[['amt', 'log_amt', 'is_low_amount', 'is_high_amount']].head(10)

# --- DOB FEATURE ENGINEERING WITH BENCHMARK DATE ---

import pandas as pd

# Convert to datetime if not already
df['dob'] = pd.to_datetime(df['dob'], errors='coerce')

# Define benchmark date (last date data was collected)
benchmark_date = pd.to_datetime('2020-12-31')

# Calculate age as of benchmark date
df['age'] = benchmark_date.year - df['dob'].dt.year

# Adjust if birthday hasn't occurred yet in 2020
mask = (
    (benchmark_date.month < df['dob'].dt.month)
    | (
        (benchmark_date.month == df['dob'].dt.month)
        & (benchmark_date.day < df['dob'].dt.day)
    )
)
df.loc[mask, 'age'] -= 1

# Categorize into age groups
def categorize_age(age):
    if pd.isna(age):
        return 'Unknown'
    elif age < 18:
        return 'Underage'
    elif 18 <= age < 30:
        return 'Young Adult'
    elif 30 <= age < 50:
        return 'Middle Aged'
    else:
        return 'Senior'

df['age_group'] = df['age'].apply(categorize_age)

# Preview results
df[['dob', 'age', 'age_group']].head()

# DropDate Of Birth (DOB) feature
df = df.drop(columns=['dob'])

# --- FEATURE ENGINEERING: GEOGRAPHICAL DISTANCE ---
import numpy as np

# Define a function to calculate Haversine distance
def haversine_distance(lat1, lon1, lat2, lon2):
    """
    Calculate the great-circle distance between two points
    on the Earth specified in decimal degrees.
    Returns distance in kilometers.
    """
    # Convert decimal degrees to radians
    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])

    # Haversine formula
    dlat = lat2 - lat1
    dlon = lon2 - lon1
    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2
    c = 2 * np.arcsin(np.sqrt(a))
    r = 6371  # Radius of Earth in kilometers
    return c * r

# Apply the function to create a new column 'distance_km'
df['distance_km'] = haversine_distance(
    df['lat'], df['long'], df['merch_lat'], df['merch_long']
)

# Display summary statistics of the new feature
print("=== Summary of New Feature: distance_km ===")
print(df['distance_km'].describe())

# --- FEATURE ENGINEERING: CUSTOMER BEHAVIOR PATTERNS ---

# 1. Total number of transactions per customer
df['cust_total_transactions'] = df.groupby('cc_num')['cc_num'].transform('count')

# 2. Average transaction amount per customer
df['cust_avg_amt'] = df.groupby('cc_num')['amt'].transform('mean')

# 3. Standard deviation of transaction amount per customer (measures spending variation)
df['cust_std_amt'] = df.groupby('cc_num')['amt'].transform('std').fillna(0)

# 4. Total number of fraud transactions per customer
df['cust_total_fraud'] = df.groupby('cc_num')['is_fraud'].transform('sum')

# 5. Customer fraud rate (ratio of fraud to total transactions)
df['cust_fraud_rate'] = df['cust_total_fraud'] / df['cust_total_transactions']

# Show a few samples to verify
print("=== Customer Behavior Features (sample) ===")
print(df[['cc_num', 'cust_total_transactions', 'cust_avg_amt', 'cust_std_amt', 'cust_total_fraud', 'cust_fraud_rate']].head())

#Data Normalization not Encoding
from sklearn.preprocessing import LabelEncoder

# Define target column name
target_col = 'is_fraud'  # <-- replace with your actual target column name

# Convert boolean-like strings to integers (0/1)
for col in df.columns:
    if df[col].dtype == 'object' and col != target_col:
        try:
            df[col] = pd.to_numeric(df[col])
        except:
            pass

# Encode remaining object columns except target
le = LabelEncoder()
for col in df.select_dtypes(include=['object']).columns:
    if col != target_col:
        df[col] = le.fit_transform(df[col].astype(str))

# Ensure target is binary (0 and 1)
df[target_col] = df[target_col].astype(int)

print("‚úÖ All categorical and object columns encoded successfully (target untouched)!")
print("Remaining non-numeric columns:", df.select_dtypes(exclude=['number']).columns.tolist())
print(df[target_col].value_counts())

# --- STEP 1: Split the dataset before any feature selection ---
from sklearn.model_selection import train_test_split

# Separate features and target
X = df.drop(columns=['is_fraud'])
y = df['is_fraud'].astype(int)

# Split data (stratify to preserve fraud ratio)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, stratify=y, random_state=42
)

print("‚úÖ Data successfully split into train/test")
print("Train shape:", X_train.shape)
print("Test shape:", X_test.shape)

# --- STEP 2: Feature selection performed only on training data ---
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.ensemble import RandomForestClassifier
import pandas as pd

# 1Ô∏è‚É£ ANOVA F-test (Top 10 features)
select_k = SelectKBest(score_func=f_classif, k=10)
select_k.fit(X_train, y_train)

anova_scores = pd.DataFrame({
    'Feature': X_train.columns,
    'F_Score': select_k.scores_
}).sort_values(by='F_Score', ascending=False)

print("=== Top 10 Features (ANOVA F-test) ===")
anova_top = anova_scores.head(10)
display(anova_top)

# 2Ô∏è‚É£ Random Forest feature importance
rf = RandomForestClassifier(random_state=42, n_estimators=100)
rf.fit(X_train, y_train)

importances = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': rf.feature_importances_
}).sort_values(by='Importance', ascending=False)

print("\n=== Top 10 Features (Random Forest Importance) ===")
rf_top = importances.head(10)
display(rf_top)

# --- STEP 3: Combine ANOVA and RF results ---
anova_features = anova_top['Feature'].tolist()
rf_features = rf_top['Feature'].tolist()

common_features = list(set(anova_features).intersection(rf_features))
combined_features = list(set(anova_features).union(rf_features))

print("=== ‚úÖ Common Top Features (Likely Strongest) ===")
print(common_features)
print("\n=== üîç Combined Feature Set (Candidate Inputs) ===")
print(combined_features)

# --- STEP 4: Forward Feature Selection using only training data ---
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.metrics import roc_auc_score

# Prepare data subsets
X_train_c = X_train[common_features]
X_test_c  = X_test[common_features]
X_train_cb = X_train[combined_features]
X_test_cb  = X_test[combined_features]

# Logistic Regression model
log_reg = LogisticRegression(max_iter=1000, solver='liblinear')

# Forward selection (Common features)
sfs_common = SequentialFeatureSelector(
    log_reg, n_features_to_select='auto', direction='forward',
    scoring='roc_auc', cv=5
)
sfs_common.fit(X_train_c, y_train)
selected_common = X_train_c.columns[sfs_common.get_support()].tolist()

# Forward selection (Combined features)
sfs_combined = SequentialFeatureSelector(
    log_reg, n_features_to_select='auto', direction='forward',
    scoring='roc_auc', cv=5
)
sfs_combined.fit(X_train_cb, y_train)
selected_combined = X_train_cb.columns[sfs_combined.get_support()].tolist()

# Evaluate both sets on the hold-out test data
log_reg.fit(X_train_c[selected_common], y_train)
auc_common = roc_auc_score(y_test, log_reg.predict_proba(X_test_c[selected_common])[:, 1])

log_reg.fit(X_train_cb[selected_combined], y_train)
auc_combined = roc_auc_score(y_test, log_reg.predict_proba(X_test_cb[selected_combined])[:, 1])

print("=== ‚úÖ Forward Selection Results ===")
print(f"Selected Features (Common Set): {selected_common}")
print(f"ROC-AUC (Common Set): {auc_common:.4f}\n")
print(f"Selected Features (Combined Set): {selected_combined}")
print(f"ROC-AUC (Combined Set): {auc_combined:.4f}")

if auc_combined > auc_common:
    print("\nüöÄ The Combined feature set performed better in forward selection.")
else:
    print("\nüéØ The Common feature set performed better or equally well.")

# --- STEP 6: Handle imbalance safely with SMOTE ---
from imblearn.over_sampling import SMOTE

# Use the better performing feature set (e.g., combined)
X_train_sel = X_train[selected_combined]
X_test_sel  = X_test[selected_combined]

print("=== Class Distribution Before SMOTE (Train) ===")
print(y_train.value_counts())

# Apply SMOTE only on training data
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_sel, y_train)

print("\n=== Class Distribution After SMOTE (Train) ===")
print(y_train_resampled.value_counts())

from sklearn.model_selection import train_test_split

# Split the SMOTE-resampled training data into:
# - training set for fitting models
# - validation set for threshold tuning
X_train_final, X_val, y_train_final, y_val = train_test_split(
    X_train_resampled,       # features after SMOTE
    y_train_resampled,       # labels after SMOTE
    test_size=0.2,           # 20% for validation
    random_state=42,
    stratify=y_train_resampled  # preserve class balance
)

print("‚úÖ Training set:", X_train_final.shape, y_train_final.shape)
print("‚úÖ Validation set:", X_val.shape, y_val.shape)

"""
##**Model Training & Evaluation**"""

# --- STEP 7: Model Training & Evaluation (LogReg, Decision Tree, Random Forest) ---
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
import matplotlib.pyplot as plt
import seaborn as sns

# ‚úÖ Define models
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000, solver='liblinear', class_weight='balanced', random_state=42),
    "Decision Tree": DecisionTreeClassifier(random_state=42, class_weight='balanced'),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
}

# ‚úÖ Store results
results = {}

for name, model in models.items():
    print(f"\n==============================")
    print(f"üöÄ Training {name}")
    print("==============================")

    # Train the model on SMOTE-balanced training set
    model.fit(X_train_final, y_train_final)  # <-- use X_train_final and y_train_final

    # Predict probabilities and labels on the original test set
    y_pred_prob = model.predict_proba(X_test_sel)[:, 1]
    y_pred = model.predict(X_test_sel)

    # Metrics
    auc = roc_auc_score(y_test, y_pred_prob)
    results[name] = auc

    print(f"\n‚úÖ ROC-AUC: {auc:.4f}")
    print("\n=== Classification Report ===")
    print(classification_report(y_test, y_pred))

    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(4,3))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.title(f"{name} - Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

# --- Compare Model ROC-AUC Scores ---
print("\n\n=== üèÜ Model Comparison (ROC-AUC Scores) ===")
for model, auc in results.items():
    print(f"{model}: {auc:.4f}")

# --- STEP 8: ROC Curves for Model Comparison ---
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, roc_auc_score

plt.figure(figsize=(8,6))

for name, model in models.items():
    # Predict probabilities on test set
    y_pred_prob = model.predict_proba(X_test_sel)[:, 1]

    # Compute ROC curve
    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)

    # Plot ROC curve with AUC in label
    plt.plot(fpr, tpr, label=f"{name} (AUC = {roc_auc_score(y_test, y_pred_prob):.4f})")

# Random guess line
plt.plot([0, 1], [0, 1], 'k--', label="Random Guess")

plt.title("ROC Curves: Logistic Regression vs Decision Tree vs Random Forest")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend()
plt.grid(True)
plt.show()

# --- STEP 3: Hyperparameter Tuning for All 3 Models using RandomizedSearchCV ---
from sklearn.model_selection import RandomizedSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
import numpy as np

# --- Logistic Regression ---
lr = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)
lr_param_dist = {
    'C': [0.01, 0.1, 1, 10],        # regularization strength
    'solver': ['liblinear', 'saga'] # suitable solvers
}

rand_search_lr = RandomizedSearchCV(
    lr,
    param_distributions=lr_param_dist,
    n_iter=4,
    scoring='roc_auc',
    cv=3,
    random_state=42,
    n_jobs=-1
)
rand_search_lr.fit(X_train_final, y_train_final)
best_lr = rand_search_lr.best_estimator_
print("Best LR params:", rand_search_lr.best_params_)
print("Best LR CV ROC-AUC:", rand_search_lr.best_score_)

# --- Decision Tree ---
dt = DecisionTreeClassifier(class_weight='balanced', random_state=42)
dt_param_dist = {
    'max_depth': [3, 5, 10, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

rand_search_dt = RandomizedSearchCV(
    dt,
    param_distributions=dt_param_dist,
    n_iter=5,
    scoring='roc_auc',
    cv=3,
    random_state=42,
    n_jobs=-1
)
rand_search_dt.fit(X_train_final, y_train_final)
best_dt = rand_search_dt.best_estimator_
print("Best DT params:", rand_search_dt.best_params_)
print("Best DT CV ROC-AUC:", rand_search_dt.best_score_)

# --- Random Forest ---
rf = RandomForestClassifier(class_weight='balanced', random_state=42)
rf_param_dist = {
    'n_estimators': [50, 100],
    'max_depth': [5, 10, None],
    'min_samples_split': [2, 5, 10],
    'max_features': ['sqrt', 'log2']
}

rand_search_rf = RandomizedSearchCV(
    rf,
    param_distributions=rf_param_dist,
    n_iter=5,
    scoring='roc_auc',
    cv=3,
    random_state=42,
    n_jobs=-1
)
rand_search_rf.fit(X_train_final, y_train_final)
best_rf = rand_search_rf.best_estimator_
print("Best RF params:", rand_search_rf.best_params_)
print("Best RF CV ROC-AUC:", rand_search_rf.best_score_)

# --- STEP 4: Evaluate Hyperparameter-Tuned Models (Before Threshold Tuning) ---

from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import matplotlib.pyplot as plt
import seaborn as sns

# Dictionary of tuned models
tuned_models = {
    "Tuned Logistic Regression": best_lr,
    "Tuned Decision Tree": best_dt,
    "Tuned Random Forest": best_rf
}

tuned_results = {}

for name, model in tuned_models.items():
    print("\n==============================")
    print(f"üìå Evaluating {name}")
    print("==============================")

    # Predict probabilities and classes
    y_pred_prob = model.predict_proba(X_test_sel)[:, 1]
    y_pred = model.predict(X_test_sel)

    # ROC-AUC Score
    auc = roc_auc_score(y_test, y_pred_prob)
    tuned_results[name] = auc
    print(f"\n‚úÖ ROC-AUC: {auc:.4f}")

    # Classification Report
    print("\n=== Classification Report ===")
    print(classification_report(y_test, y_pred))

    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(4, 3))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.title(f"{name} - Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

# --- Compare ROC-AUC of Tuned Models ---
print("\n\n=== üèÜ Tuned Model Comparison (ROC-AUC Scores) ===")
for model, auc in tuned_results.items():
    print(f"{model}: {auc:.4f}")

# --- STEP 5: Threshold Tuning for All Tuned Models (Using Validation Set) ---
import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score

# ‚úÖ Apply same selected features to validation set (NO refitting ‚Üí no leakage)
X_val_sel = X_val[selected_combined] # Corrected from selected_features

tuned_models = {
    "Tuned Logistic Regression": best_lr,
    "Tuned Decision Tree": best_dt,
    "Tuned Random Forest": best_rf
}

threshold_results = {}

for name, model in tuned_models.items():
    print("\n==============================")
    print(f"üéØ Threshold Tuning: {name}")
    print("==============================")

    # ‚úÖ Predict probabilities on validation set ONLY
    y_prob = model.predict_proba(X_val_sel)[:, 1]

    best_thr = 0
    best_f1 = 0
    best_precision = 0
    best_recall = 0

    # ‚úÖ Loop through thresholds 0.01 - 0.99
    for thr in np.arange(0.01, 1.0, 0.01):
        y_pred_thr = (y_prob >= thr).astype(int)

        precision = precision_score(y_val, y_pred_thr, zero_division=0)
        recall = recall_score(y_val, y_pred_thr, zero_division=0)
        f1 = f1_score(y_val, y_pred_thr, zero_division=0)

        if f1 > best_f1:
            best_f1 = f1
            best_thr = thr
            best_precision = precision
            best_recall = recall

    # Store the best metrics for the current model
    threshold_results[name] = {
        "Best Threshold": best_thr,
        "Precision (Fraud)": best_precision,
        "Recall (Fraud)": best_recall,
        "F1 (Fraud)": best_f1
    }

    print(f"üî• Best Threshold: {best_thr:.3f}")
    print(f"Precision (Fraud): {best_precision:.4f}")
    print(f"Recall (Fraud): {best_recall:.4f}")
    print(f"F1-score (Fraud): {best_f1:.4f}")

# --- Summary of Threshold Tuning Results ---
print("\n\n=== üèÜ Threshold Tuning Results (All Tuned Models) ===")
for model, metrics in threshold_results.items():
    print(f"\n{model}")
    for k, v in metrics.items():
        print(f"{k}: {v:.4f}")

# --- STEP 6: Final Evaluation on Test Set Using Best Thresholds ---
from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

final_results = {}

for name, model in tuned_models.items():
    print("\n==============================")
    print(f"‚úÖ Final Test Evaluation: {name}")
    print("==============================")

    # Get best threshold from validation tuning
    best_thr = threshold_results[name]["Best Threshold"]

    # Predict probabilities on TEST set
    y_prob_test = model.predict_proba(X_test_sel)[:, 1]

    # Apply optimized threshold
    y_pred_test = (y_prob_test >= best_thr).astype(int)

    # Compute metrics
    precision = precision_score(y_test, y_pred_test, zero_division=0)
    recall = recall_score(y_test, y_pred_test, zero_division=0)
    f1 = f1_score(y_test, y_pred_test, zero_division=0)

    final_results[name] = {
        "Best Threshold": best_thr,
        "Precision (Fraud)": precision,
        "Recall (Fraud)": recall,
        "F1 (Fraud)": f1
    }

    print(f"üî• Using Threshold: {best_thr:.3f}")
    print(classification_report(y_test, y_pred_test))

    # Confusion matrix visualization
    cm = confusion_matrix(y_test, y_pred_test)
    plt.figure(figsize=(4, 3))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', cbar=False)
    plt.title(f"{name} - Confusion Matrix (Test Set)")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

# --- Summary of Final Performance ---
print("\n\n=== üèÅ Final Model Performance on Test Set (Using Tuned Thresholds) ===")
for model, metrics in final_results.items():
    print(f"\n{model}")
    for k, v in metrics.items():
        print(f"{k}: {v:.4f}")

# --- STEP 6: Model Saving & Deployment Preparation
import joblib
import numpy as np

# --------------------------------------------------
# 1Ô∏è‚É£ Save the Final Model (Decision Tree Selected)
# --------------------------------------------------

final_model = best_dt
joblib.dump(final_model, "final_decision_tree_model.pkl")
print("‚úÖ Final model saved: final_decision_tree_model.pkl")


# --------------------------------------------------
# 2Ô∏è‚É£ Save the Best Threshold
# --------------------------------------------------

best_threshold = threshold_results["Tuned Decision Tree"]["Best Threshold"]
joblib.dump(best_threshold, "fraud_best_threshold.pkl")
print("‚úÖ Best threshold saved: fraud_best_threshold.pkl")


# --------------------------------------------------
# 3Ô∏è‚É£ Save the Selected Features
# --------------------------------------------------
# Using the `selected_combined` from forward feature selection
joblib.dump(selected_combined, "selected_features.pkl")
print("‚úÖ Selected features saved: selected_features.pkl")


# --------------------------------------------------
# 4Ô∏è‚É£ Define Prediction Function for Deployment
# --------------------------------------------------

def predict_with_threshold(model, threshold, X):
    """
    Predict fraud using model probability and a saved threshold.
    Returns predicted class and probability.
    """
    prob = model.predict_proba(X)[:, 1]
    preds = (prob >= threshold).astype(int)
    return preds, prob


print("‚úÖ predict_with_threshold() function created successfully.")


# --------------------------------------------------
# 5Ô∏è‚É£ Deployment Simulation (Loading + Predicting)
# --------------------------------------------------

# Load files (exactly how production systems use them)
loaded_model = joblib.load("final_decision_tree_model.pkl")
loaded_threshold = joblib.load("fraud_best_threshold.pkl")

# Example prediction call (commented out for clean deployment)
# preds, probs = predict_with_threshold(loaded_model, loaded_threshold, X_test_sel[:5])
# print(preds, probs)

print("‚úÖ Deployment files and function are ready for API integration.")
# --------------------------------------------------
# END OF STEP 8
# --------------------------------------------------